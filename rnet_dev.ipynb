{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import layers\n",
    "from util import collate_fn, SQuAD\n",
    "import util\n",
    "from util import masked_softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import BiDAF, RNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load sample batch\n",
    "* Batch size 64 (64 training examples)\n",
    "* Each word is padded to 16 character"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "cw_idxs = torch.load('data_dev/cw_idxs.pt') # torch.Size([64, 293])\n",
    "cc_idxs = torch.load('data_dev/cc_idxs.pt') # torch.Size([64, 293, 16])\n",
    "qw_idxs = torch.load('data_dev/qw_idxs.pt') # torch.Size([64, 29])\n",
    "qc_idxs = torch.load('data_dev/qc_idxs.pt') # torch.Size([64, 29, 16])\n",
    "y1 = torch.load('data_dev/y1.pt') # torch.Size([64])\n",
    "y2 = torch.load('data_dev/y2.pt') # torch.Size([64])\n",
    "ids = torch.load('data_dev/ids.pt') # torch.Size([64])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_mask = torch.zeros_like(cw_idxs) != cw_idxs\n",
    "q_mask = torch.zeros_like(qw_idxs) != qw_idxs\n",
    "c_len, q_len = c_mask.sum(-1), q_mask.sum(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([17, 18, 19, 10,  7, 12,  3,  0,  0,  0,  0,  0,  0,  0,  0,  0])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cc_idxs[0][0].size()\n",
    "cc_idxs[0][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1,  1,  1,  ...,  1,  1,  1],\n",
       "        [40,  6,  0,  ...,  0,  0,  0],\n",
       "        [17, 18, 19,  ...,  0,  0,  0],\n",
       "        ...,\n",
       "        [ 0,  0,  0,  ...,  0,  0,  0],\n",
       "        [ 0,  0,  0,  ...,  0,  0,  0],\n",
       "        [ 0,  0,  0,  ...,  0,  0,  0]])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cc_idxs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = cw_idxs.size()[0]\n",
    "hidden_size = 100 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vectors = util.torch_from_json(\"data/word_emb.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([88714, 300])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_vectors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "char_vectors = util.torch_from_json(\"data/char_emb.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1376, 64])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "char_vectors.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### R-Net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Embedding\n",
    "R-net is built with four major components. The first step is to encode both passage P = {wtP }nt=1 and question Q = {wtQ}mt=1 with a bi-directional RNN, which transform word embeddings ({ePt }nt=1, {eQt }mt=1) and character\n",
    "embeddings ({cPt }nt=1 , {cQt }mt=1 ) to new encoded representation ({uPt }nt=1 , {uQt }mt=1 ). The character and word embeddings are concatenated before entering the bi-directional RNN. Because of efficiency, the original paper chooses GRU cell instead of LSTM cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharEmbedding(nn.Module):\n",
    "    def __init__(self, char_vectors, e_char, e_word):\n",
    "        \"\"\"\n",
    "        The character-level embeddings are generated by taking the \n",
    "        final hidden states of a bi-directional recurrent neural \n",
    "        network (RNN) applied to embeddings of characters in the token.\n",
    "        \"\"\"\n",
    "        super(CharEmbedding, self).__init__()\n",
    "        \n",
    "        self.e_char = e_char\n",
    "        self.e_word = e_word\n",
    "        \n",
    "        self.embeddings = nn.Embedding.from_pretrained(char_vectors) \n",
    "        \n",
    "        self.encoder = nn.LSTM(input_size=e_char, \n",
    "                               bidirectional=True, \n",
    "                               hidden_size=int(e_word / 2),\n",
    "                               batch_first=True)\n",
    "\n",
    "    def forward(self, input):\n",
    "        \"\"\"\n",
    "        @param input (Tensor): indices of character batch (batch_size, sentence_length, max_word_length)\n",
    "        @returns last_hidden (Tensor): Tensor of shape (batch_size, sentence_length, 2 * e_word)\n",
    "        \"\"\"\n",
    "        x_emb = self.embeddings(input)\n",
    "        batch_size, sentence_length, max_word_length, e_char = x_emb.shape\n",
    "        \n",
    "        # reshape to (batch_size * sentence_length, max_word_length, e_char)\n",
    "        x_emb = x_emb.reshape(batch_size * sentence_length, max_word_length, e_char)\n",
    "        \n",
    "        # last_hidden: (2, batch_size * sentence_length, e_word)\n",
    "        _, (last_hidden, _) = self.encoder(x_emb)\n",
    "        \n",
    "        # concate the two direction hidden states into shape (batch_size * sentence_length, 2 * e_word)\n",
    "        last_hidden = torch.cat([last_hidden[0], last_hidden[1]], dim=1)\n",
    "        \n",
    "        # break apart dimension to (batch_size, sentence_length, 2 * e_word)\n",
    "        last_hidden = last_hidden.reshape(batch_size, sentence_length, -1)\n",
    "        return last_hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 293])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# charater based embedding\n",
    "char_vectors = util.torch_from_json(\"data/char_emb.json\")\n",
    "char_emb = CharEmbedding(char_vectors, e_char=64, e_word=200)\n",
    "\n",
    "cc_mask = torch.zeros_like(cc_idxs) != cc_idxs\n",
    "qc_mask = torch.zeros_like(qc_idxs) != qc_idxs\n",
    "cc_len, qc_len = cc_mask.sum(-1), qc_mask.sum(-1)\n",
    "        \n",
    "qc = char_emb.forward(qc_idxs)\n",
    "# cc = char_emb.forward(cc_idxs, cc_len)\n",
    "# qc.shape\n",
    "\n",
    "cc_len.shape\n",
    "cc_len.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 29, 300])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lookup word embedding\n",
    "embed = nn.Embedding.from_pretrained(word_vectors) \n",
    "\n",
    "qw = embed(qw_idxs)\n",
    "cw = embed(cw_idxs)\n",
    "qw.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 29, 500])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# concatenate the two embedding\n",
    "q = torch.cat((qc, qw), dim=2)\n",
    "c = torch.cat((cc, cw), dim=2)\n",
    "q.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_size, h_size, dropout=0):\n",
    "        super(Encoder, self).__init__()\n",
    "        \"\"\"\n",
    "        This encoder takes concatenated embeddings of context (or question)\n",
    "        and output a single hidden state representing the context (or question)\n",
    "        \n",
    "        @param input_size (int): input size is the sum of word embedding size \n",
    "            and character-based word embedding size (output of CharEmbedding module)\n",
    "            this number is kept as the standard reference in all subsequent layers\n",
    "        @param h_size (int): size of hidden state, also the output size\n",
    "        \"\"\"\n",
    "        self.input_size = input_size\n",
    "        self.h_size = h_size\n",
    "        self.out_size = 2 * h_size\n",
    "        self.gru = nn.GRU(input_size=input_size, \n",
    "            bidirectional=True, \n",
    "            hidden_size=h_size,\n",
    "            batch_first=True)\n",
    "        \n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "    def forward(self, input, lengths):\n",
    "        \"\"\"\n",
    "        @param input (Tensor): concatenated embedding of character & word embeddings,\n",
    "            shape (batch_size, sentence_length, embedding_size)\n",
    "        @return last_hidden (Tensor): hidden state of every word of text, \n",
    "            shape (batch_size, sentence_length, 2 * h_size)\n",
    "        \"\"\"\n",
    "        batch, sentence_length, embedding_size = input.size()\n",
    "        \n",
    "        # Sort by length and pack sequence for RNN\n",
    "        lengths, sort_idx = lengths.sort(0, descending=True)\n",
    "        input = input[sort_idx]     # (batch_size, seq_len, input_size)\n",
    "        input = pack_padded_sequence(input, lengths, batch_first=True)\n",
    "        \n",
    "        # last_hidden: (batch_size, sentence_length, 2 * h_size)\n",
    "        hiddens, last_hidden = self.gru(input) \n",
    "        \n",
    "        # Unpack and reverse sort\n",
    "        hiddens, _ = pad_packed_sequence(hiddens, batch_first=True, total_length=sentence_length)\n",
    "        _, unsort_idx = sort_idx.sort(0)\n",
    "        hiddens = hiddens[unsort_idx]   # (batch_size, seq_len, 2 * hidden_size)\n",
    "        \n",
    "        # apply dropout\n",
    "        return self.dropout(hiddens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 293, 500])"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 293, 400])"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder = Encoder(input_size=500, h_size=200)\n",
    "uc = encoder.forward(c, c_len)\n",
    "uq = encoder.forward(q, q_len)\n",
    "uc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 29, 400])"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uq.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Gated Attention Based Recurrent Networks\n",
    "The second component, Gated Attention-Based Recurrent Networks, modifies the representation of each passage word to become aware of the question ({vtP }nt=1). For passage word at step t, attention ct is aggregated over the entire question uQ. A signmoid input gate is added to attenuate the cell state ct, in order to capture the relation between current context word uPt and the entire question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GatedAttn(nn.Module):\n",
    "    def __init__(self, input_size, h_size, dropout=0):\n",
    "        super(GatedAttn, self).__init__()\n",
    "        \n",
    "        hidden_size = h_size\n",
    "        self.hidden_size = hidden_size * 2\n",
    "        self.input_size = input_size\n",
    "        self.out_size = 2 * h_size\n",
    "        \n",
    "        self.gru = nn.GRUCell(input_size=input_size * 2, hidden_size=h_size * 2)\n",
    "        \n",
    "        self.Wp = nn.Linear(in_features=input_size * 2, \n",
    "                            out_features=h_size * 2, \n",
    "                            bias=False)\n",
    "        \n",
    "        self.Wq = nn.Linear(in_features=input_size * 2, \n",
    "                            out_features=h_size * 2, \n",
    "                            bias=False)\n",
    "        \n",
    "        self.Wv = nn.Linear(in_features=h_size * 2, \n",
    "                            out_features=h_size * 2, \n",
    "                            bias=False)\n",
    "        \n",
    "        self.Wg = nn.Linear(in_features=input_size * 4, \n",
    "                            out_features=input_size * 4, \n",
    "                            bias=False)\n",
    "\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "    \n",
    "    def forward(self, up, uq, c_len):\n",
    "        up = up.permute(1, 0, 2) # [n, batch_size, 2 * h_size]\n",
    "        uq = uq.permute(1, 0, 2) # [m, batch_size, 2 * h_size]\n",
    "        (n, batch_size, _) = up.size()\n",
    "        (m, _, _) = uq.size()\n",
    "\n",
    "        Up = up\n",
    "        Uq = uq\n",
    "        \n",
    "        vs = torch.zeros(n, batch_size, self.out_size).to(device)\n",
    "        v = torch.randn(batch_size, self.hidden_size).to(device)\n",
    "        V = torch.randn(batch_size, self.hidden_size, 1).to(device)\n",
    "        \n",
    "        # [64, 29, 400]\n",
    "        Uq_ = Uq.permute([1, 0, 2])\n",
    "        for i in range(n):\n",
    "            Wup = self.Wp(Up[i]) # [64, 400] -> [64, h_size]\n",
    "            Wuq = self.Wq(Uq) # [29, 64, 400] -> [29, 64, h_size]\n",
    "            Wvv = self.Wv(v) # [batch_size, h_size] -> [batch_size, h_size]\n",
    "            x = torch.tanh(Wup + Wuq + Wvv) # (29, 64, 400)\n",
    "            \n",
    "            x = x.permute([1, 0, 2]) # (64, 29, 400)\n",
    "            s = torch.bmm(x, V).squeeze() # (64, 29)\n",
    "            \n",
    "            a = torch.softmax(s, dim=1).unsqueeze(1) # [64, 1, 29]\n",
    "#             a = masked_softmax(s, q_len[i], dim=1).unsqueeze(1) # [64, 1, 29]\n",
    "\n",
    "            c = torch.bmm(a, Uq_).squeeze() # (64, 400)\n",
    "            r = torch.cat([Up[i], c], dim=1) # (64, 800)\n",
    "            g = torch.sigmoid(self.Wg(r)) # (64, 800)\n",
    "            r_ = torch.mul(g, r) # element-wise mult\n",
    "            \n",
    "            c_ = r_[:, self.input_size*2:]\n",
    "            v = self.gru(c_, v)\n",
    "            vs[i] = v\n",
    "            del Wup, Wuq, Wvv, x, a, s, c, g, r, r_, c_\n",
    "        del up, uq, Up, Uq, Uq_\n",
    "        vs = self.dropout(vs)\n",
    "        return vs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "gatedAttn = GatedAttn(input_size=200, h_size=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([293, 64, 400])"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vp = gatedAttn.forward(uc, uq, c_len)\n",
    "vp.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Self Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttn(nn.Module):\n",
    "    def __init__(self, in_size, dropout=0):\n",
    "        super(SelfAttn, self).__init__()\n",
    "        self.hidden_size = in_size\n",
    "        self.in_size = in_size\n",
    "        \n",
    "        self.gru = nn.GRUCell(input_size=in_size, hidden_size=self.hidden_size)\n",
    "        \n",
    "        self.Wp = nn.Linear(self.in_size, self.hidden_size, bias=False)\n",
    "        self.Wp_ = nn.Linear(self.in_size, self.hidden_size, bias=False)\n",
    "        \n",
    "        self.out_size = self.hidden_size\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "    def forward(self, v):\n",
    "        (l, batch_size, _) = v.size()\n",
    "        h = torch.randn(batch_size, self.hidden_size).to(device)\n",
    "        V = torch.randn(batch_size, self.hidden_size, 1).to(device)\n",
    "        hs = torch.zeros(l, batch_size, self.out_size).to(device)\n",
    "        \n",
    "        for i in range(l):\n",
    "            Wpv = self.Wp(v[i])\n",
    "            Wpv_ = self.Wp_(v)\n",
    "            x = torch.tanh(Wpv + Wpv_)\n",
    "            x = x.permute([1, 0, 2])\n",
    "            s = torch.bmm(x, V)\n",
    "            s = torch.squeeze(s, 2)\n",
    "            a = torch.softmax(s, 1).unsqueeze(1)\n",
    "            c = torch.bmm(a, v.permute([1, 0, 2])).squeeze()\n",
    "            h = self.gru(c, h)\n",
    "            hs[i] = h\n",
    "            del Wpv, Wpv_, x, s, a, c\n",
    "        hs = self.dropout(hs)\n",
    "        del h, v\n",
    "        return hs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "selfAttn = SelfAttn(in_size=400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "400"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selfAttn.out_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([293, 64, 400])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c = selfAttn.forward(vp)\n",
    "c.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Pointer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Pointer(nn.Module):\n",
    "    def __init__(self, in_size1, in_size2):\n",
    "        super(Pointer, self).__init__()\n",
    "        self.hidden_size = in_size2\n",
    "        self.in_size1 = in_size1\n",
    "        self.in_size2 = in_size2\n",
    "        self.gru = nn.GRUCell(input_size=in_size1, hidden_size=self.hidden_size)\n",
    "        # Wu uses bias. See formula (11). Maybe Vr is just a bias.\n",
    "        self.Wu = nn.Linear(self.in_size2, self.hidden_size, bias=True)\n",
    "        self.Wh = nn.Linear(self.in_size1, self.hidden_size, bias=False)\n",
    "        self.Wha = nn.Linear(self.in_size2, self.hidden_size, bias=False)\n",
    "        self.out_size = 1\n",
    "\n",
    "    def forward(self, h, u):\n",
    "        \"\"\"\n",
    "        self matching output, \n",
    "        Uq: [64, 29, 400]\n",
    "        \"\"\"\n",
    "        (lp, batch_size, _) = h.size()\n",
    "  \n",
    "        u = u.permute(1, 0, 2)\n",
    "        (lq, _, _) = u.size()\n",
    "        v = torch.randn(batch_size, self.hidden_size, 1).to(device)\n",
    "        u_ = u.permute([1,0,2]) # (m, batch_size, h_size * 2)\n",
    "        h_ = h.permute([1,0,2]) # (m, batch_size, h_size * 2)\n",
    "        x = torch.tanh(self.Wu(u)).permute([1, 0, 2])\n",
    "        s = torch.bmm(x, v)\n",
    "        s = torch.squeeze(s, 2)\n",
    "        a = torch.softmax(s, 1).unsqueeze(1)\n",
    "        r = torch.bmm(a, u_).squeeze()\n",
    "        x = torch.tanh(self.Wh(h)+self.Wha(r)).permute([1, 0, 2])\n",
    "        s = torch.bmm(x, v)\n",
    "        s = torch.squeeze(s)\n",
    "        p1 = torch.softmax(s, 1)\n",
    "        c = torch.bmm(p1.unsqueeze(1), h_).squeeze()\n",
    "        r = self.gru(c, r)\n",
    "        x = torch.tanh(self.Wh(h) + self.Wha(r)).permute([1, 0, 2])\n",
    "        s = torch.bmm(x, v)\n",
    "        s = torch.squeeze(s)\n",
    "        p2 = torch.softmax(s, 1)\n",
    "        return p1, p2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 29, 400])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uq.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "pointer = Pointer(in_size1=400, in_size2=400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "p1, p2 = pointer(c, uq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 293])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 293])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p2.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Put together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNet(nn.Module):\n",
    "    def __init__(self, word_vectors, char_vectors, drop_prob=0):\n",
    "        super(RNet, self).__init__()\n",
    "        self.word_emb = nn.Embedding.from_pretrained(word_vectors)\n",
    "        self.char_emb = CharEmbedding(char_vectors, e_char=64, e_word=200)\n",
    "        \n",
    "        self.encoder = Encoder(input_size=500, h_size=200)\n",
    "        self.gatedAttn = GatedAttn(input_size=200, h_size=200)\n",
    "        self.selfAttn = SelfAttn(self.gatedAttn.out_size)\n",
    "        self.pointer = Pointer(self.selfAttn.out_size, self.encoder.out_size)\n",
    "\n",
    "    # wemb of P, cemb of P, w of Q, c of Q, Answer\n",
    "    def forward(self, Pcw_idxs, qw_idxs, cc_idxs, qc_idxs):\n",
    "        qc = self.char_emb.forward(qc_idxs)\n",
    "        cc = self.char_emb.forward(cc_idxs)\n",
    "\n",
    "        qw = self.word_emb(qw_idxs)\n",
    "        cw = self.word_emb(cw_idxs)\n",
    "\n",
    "        Q = torch.cat((qc, qw), dim=2)\n",
    "        P = torch.cat((cc, cw), dim=2)\n",
    "\n",
    "        Up = self.encoder.forward(P)\n",
    "        Uq = self.encoder.forward(Q)\n",
    "        \n",
    "        v = self.gatedAttn.forward(Up, Uq)\n",
    "        torch.cuda.empty_cache()\n",
    "        h = self.selfAttn(v)\n",
    "        p1, p2 = self.pointer(h, Uq)\n",
    "        return p1, p2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnet = RNet(word_vectors, char_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 293, 400])\n",
      "torch.Size([64, 29, 400])\n",
      "torch.Size([293, 64, 400])\n",
      "torch.Size([293, 64, 400])\n"
     ]
    }
   ],
   "source": [
    "p1, p2 = rnet.forward(cw_idxs, qw_idxs, cc_idxs, qc_idxs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 293])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 293])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p2.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
